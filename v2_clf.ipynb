{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "import tika\n",
    "from tika import parser\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['data/AS_', 'data/Ml_', 'data/Bd_', 'data/Hd_']\n",
    "dados = []\n",
    "num_data = []\n",
    "for elem in range(0, len(base)):\n",
    "    count = 0\n",
    "    if elem != 0:\n",
    "        final = 10\n",
    "    else:\n",
    "        final = 34\n",
    "        \n",
    "    for i in range(1, final):\n",
    "        terminal = base[elem] + str(i) + '.pdf'\n",
    "        dados.append(terminal)\n",
    "        count+=1\n",
    "    \n",
    "    num_data.append(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['data/Ml_', 'data/Bd_', 'data/Hd_', 'data/AS_']\n",
    "dados = []\n",
    "num_data = []\n",
    "for elem in range(0, len(base)):\n",
    "    count = 0\n",
    "    final = 10    \n",
    "    for i in range(1, final):\n",
    "        terminal = base[elem] + str(i) + '.pdf'\n",
    "        dados.append(terminal)\n",
    "        count+=1\n",
    "    \n",
    "    num_data.append(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/Ml_1.pdf', 'data/Ml_2.pdf', 'data/Ml_3.pdf', 'data/Ml_4.pdf', 'data/Ml_5.pdf', 'data/Ml_6.pdf', 'data/Ml_7.pdf', 'data/Ml_8.pdf', 'data/Ml_9.pdf', 'data/Bd_1.pdf', 'data/Bd_2.pdf', 'data/Bd_3.pdf', 'data/Bd_4.pdf', 'data/Bd_5.pdf', 'data/Bd_6.pdf', 'data/Bd_7.pdf', 'data/Bd_8.pdf', 'data/Bd_9.pdf', 'data/Hd_1.pdf', 'data/Hd_2.pdf', 'data/Hd_3.pdf', 'data/Hd_4.pdf', 'data/Hd_5.pdf', 'data/Hd_6.pdf', 'data/Hd_7.pdf', 'data/Hd_8.pdf', 'data/Hd_9.pdf', 'data/AS_1.pdf', 'data/AS_2.pdf', 'data/AS_3.pdf', 'data/AS_4.pdf', 'data/AS_5.pdf', 'data/AS_6.pdf', 'data/AS_7.pdf', 'data/AS_8.pdf', 'data/AS_9.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['data/Ml_1.pdf', 'data/Ml_2.pdf', 'data/Ml_3.pdf', 'data/Ml_4.pdf', 'data/Ml_5.pdf', 'data/Ml_6.pdf', 'data/Ml_7.pdf']\n",
    "biblioteca = []\n",
    "tika.initVM()\n",
    "for i in range(0, len(dados)):\n",
    "    \n",
    "    parsed = parser.from_file(dados[i])\n",
    "    metadados = parsed[\"metadata\"]\n",
    "    texto = parsed[\"content\"]\n",
    "    biblioteca.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(biblioteca))\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open (\"artigo2.txt\", \"w\")\n",
    "# f.write (str(texto))\n",
    "# f.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f = open (\"artigo2.txt\", \"r\")\n",
    "# texto = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_clear(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    #remove links and mark\n",
    "    data = [re.sub('(https:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(http:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(www.:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # remove spaces in begining and end\n",
    "    data = [re.sub(' +', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s): \n",
    "    a = 0\n",
    "    # initialization of string to \"\" \n",
    "    new = \"\" \n",
    "  \n",
    "    # traverse in the string  \n",
    "    for x in s: \n",
    "        new += x + ' '\n",
    "        a+=1\n",
    "  \n",
    "    # return string  \n",
    "    return new, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleared = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_char = 0\n",
    "for i in range(0, len(biblioteca)):\n",
    "    texto = biblioteca[i]\n",
    "    data = split_pdf(texto)\n",
    "    data = first_clear(data)\n",
    "    d, a = convert(data)\n",
    "    \n",
    "    total_char += a\n",
    "    data_cleared.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "292303\n"
     ]
    }
   ],
   "source": [
    "print(len(data_cleared))\n",
    "print(total_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_cleared)\n",
    "df = df.rename(columns={0: \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Comparative Study on Feature Selection in T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fqs003 183..196 Detecting authorship deceptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Machine Learning Methods Demonstrate the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fqq001 215..224 A comparative study of machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fqi039 259..274 A New Approach to the Study o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   A Comparative Study on Feature Selection in T...\n",
       "1   fqs003 183..196 Detecting authorship deceptio...\n",
       "2   New Machine Learning Methods Demonstrate the ...\n",
       "3   fqq001 215..224 A comparative study of machin...\n",
       "4   fqi039 259..274 A New Approach to the Study o..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(0, len(dados)):\n",
    "    if 'AS' in str(dados[i]):\n",
    "        l.append('Analyze_Statistics')\n",
    "    elif 'Ml' in str(dados[i]):\n",
    "        l.append('Machine_Learning')\n",
    "    elif 'Bd' in str(dados[i]):\n",
    "        l.append('Big_Data')\n",
    "    else:\n",
    "        l.append('Digital_Humanities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classe'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Comparative Study on Feature Selection in T...</td>\n",
       "      <td>Machine_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fqs003 183..196 Detecting authorship deceptio...</td>\n",
       "      <td>Machine_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Machine Learning Methods Demonstrate the ...</td>\n",
       "      <td>Machine_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fqq001 215..224 A comparative study of machin...</td>\n",
       "      <td>Machine_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fqi039 259..274 A New Approach to the Study o...</td>\n",
       "      <td>Machine_Learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            classe\n",
       "0   A Comparative Study on Feature Selection in T...  Machine_Learning\n",
       "1   fqs003 183..196 Detecting authorship deceptio...  Machine_Learning\n",
       "2   New Machine Learning Methods Demonstrate the ...  Machine_Learning\n",
       "3   fqq001 215..224 A comparative study of machin...  Machine_Learning\n",
       "4   fqi039 259..274 A New Approach to the Study o...  Machine_Learning"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.classe]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.classe]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     ([acmcs00.tex, machine, learning, in, automate...\n",
       "17    ([foundations, and, trends, in, information, r...\n",
       "9     ([unified, approach, to, mapping, and, cluster...\n",
       "34    ([microsoft, word, dabagh.doc, metodološki, zv...\n",
       "0     ([comparative, study, on, feature, selection, ...\n",
       "4     ([fqi039, 259..274, new, approach, to, the, st...\n",
       "29    ([fqi063, 311..326, tool, for, literary, studi...\n",
       "15    ([doi, 10.1126/science.1199644, 176, 2011, 331...\n",
       "19    ([9-25_1, golumbia, 156-76, .indd, volume, 25,...\n",
       "5     ([using, markov, chains, for, identification, ...\n",
       "11    ([fqs010, 139..154, automatic, prediction, of,...\n",
       "1     ([fqs003, 183..196, detecting, authorship, dec...\n",
       "24    ([8-25_1, lennon, 132-55, .indd, volume, 25, n...\n",
       "2     ([new, machine, learning, methods, demonstrate...\n",
       "33    ([journal, of, machine, learning, research, 20...\n",
       "3     ([fqq001, 215..224, comparative, study, of, ma...\n",
       "32    ([pone.0087908, 1..10, computational, approach...\n",
       "23    ([signs, symbols, and, discourses, new, direct...\n",
       "27    ([accurate, methods, for, the, statistics, of,...\n",
       "10    ([microsoft, word, forman-jmlr-fxselectionstud...\n",
       "22    ([fqm042, 73..84, killer, applications, in, di...\n",
       "18    ([confronting, the, digital, or, how, academic...\n",
       "25    ([11/8/12, dhq, digital, humanities, quarterly...\n",
       "6     ([op-llcj120026, 229..236, do, birds, of, feat...\n",
       "20    ([distant, reading, and, discourse, analysis, ...\n",
       "7     ([large-scale, classification, of, fine-art, p...\n",
       "14    ([probabilistic, topic, models, review, articl...\n",
       "28    ([11/17/2019, companion, to, digital, literary...\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 45661.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=1, vector_size=500, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 95170.59it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 140985.01it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 93059.04it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 94863.10it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 53921.26it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 67650.06it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 59735.76it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 69821.95it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 117793.89it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 119471.53it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 13834.43it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 15675.46it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 28651.02it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 17734.90it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 96658.86it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 10318.09it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 95635.60it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 70365.80it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 64210.23it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 93802.33it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 19878.22it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 15110.72it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 17580.91it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 138818.57it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 103199.04it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 25902.19it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 109655.01it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 94178.44it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 150951.81it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 228928.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 383 ms, total: 31.5 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5\n",
      "Testing F1 score: 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
