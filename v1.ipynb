{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "import tika\n",
    "from tika import parser\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['data/AS_', 'data/Ml_', 'data/Bd_', 'data/Hd_']\n",
    "data = []\n",
    "num_data = []\n",
    "for elem in range(0, len(base)):\n",
    "    count = 0\n",
    "    if elem != 0:\n",
    "        final = 10\n",
    "    else:\n",
    "        final = 34\n",
    "        \n",
    "    for i in range(1, final):\n",
    "        terminal = base[elem] + str(i) + '.pdf'\n",
    "        data.append(terminal)\n",
    "        count+=1\n",
    "    \n",
    "    num_data.append(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/AS_1.pdf',\n",
       " 'data/AS_2.pdf',\n",
       " 'data/AS_3.pdf',\n",
       " 'data/AS_4.pdf',\n",
       " 'data/AS_5.pdf',\n",
       " 'data/AS_6.pdf',\n",
       " 'data/AS_7.pdf',\n",
       " 'data/AS_8.pdf',\n",
       " 'data/AS_9.pdf',\n",
       " 'data/AS_10.pdf',\n",
       " 'data/AS_11.pdf',\n",
       " 'data/AS_12.pdf',\n",
       " 'data/AS_13.pdf',\n",
       " 'data/AS_14.pdf',\n",
       " 'data/AS_15.pdf',\n",
       " 'data/AS_16.pdf',\n",
       " 'data/AS_17.pdf',\n",
       " 'data/AS_18.pdf',\n",
       " 'data/AS_19.pdf',\n",
       " 'data/AS_20.pdf',\n",
       " 'data/AS_21.pdf',\n",
       " 'data/AS_22.pdf',\n",
       " 'data/AS_23.pdf',\n",
       " 'data/AS_24.pdf',\n",
       " 'data/AS_25.pdf',\n",
       " 'data/AS_26.pdf',\n",
       " 'data/AS_27.pdf',\n",
       " 'data/AS_28.pdf',\n",
       " 'data/AS_29.pdf',\n",
       " 'data/AS_30.pdf',\n",
       " 'data/AS_31.pdf',\n",
       " 'data/AS_32.pdf',\n",
       " 'data/AS_33.pdf',\n",
       " 'data/Ml_1.pdf',\n",
       " 'data/Ml_2.pdf',\n",
       " 'data/Ml_3.pdf',\n",
       " 'data/Ml_4.pdf',\n",
       " 'data/Ml_5.pdf',\n",
       " 'data/Ml_6.pdf',\n",
       " 'data/Ml_7.pdf',\n",
       " 'data/Ml_8.pdf',\n",
       " 'data/Ml_9.pdf',\n",
       " 'data/Bd_1.pdf',\n",
       " 'data/Bd_2.pdf',\n",
       " 'data/Bd_3.pdf',\n",
       " 'data/Bd_4.pdf',\n",
       " 'data/Bd_5.pdf',\n",
       " 'data/Bd_6.pdf',\n",
       " 'data/Bd_7.pdf',\n",
       " 'data/Bd_8.pdf',\n",
       " 'data/Bd_9.pdf',\n",
       " 'data/Hd_1.pdf',\n",
       " 'data/Hd_2.pdf',\n",
       " 'data/Hd_3.pdf',\n",
       " 'data/Hd_4.pdf',\n",
       " 'data/Hd_5.pdf',\n",
       " 'data/Hd_6.pdf',\n",
       " 'data/Hd_7.pdf',\n",
       " 'data/Hd_8.pdf',\n",
       " 'data/Hd_9.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['data/Ml_1.pdf', 'data/Ml_2.pdf', 'data/Ml_3.pdf', 'data/Ml_4.pdf', 'data/Ml_5.pdf', 'data/Ml_6.pdf', 'data/Ml_7.pdf']\n",
    "biblioteca = []\n",
    "tika.initVM()\n",
    "for i in range(0, len(data)):\n",
    "    \n",
    "    parsed = parser.from_file(data[i])\n",
    "    metadados = parsed[\"metadata\"]\n",
    "    texto = parsed[\"content\"]\n",
    "    biblioteca.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(biblioteca))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open (\"artigo2.txt\", \"w\")\n",
    "# f.write (str(texto))\n",
    "# f.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f = open (\"artigo2.txt\", \"r\")\n",
    "# texto = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_clear(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    #remove links and mark\n",
    "    data = [re.sub('(https:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(http:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(www.:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # remove spaces in begining and end\n",
    "    data = [re.sub(' +', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s): \n",
    "    a = 0\n",
    "    # initialization of string to \"\" \n",
    "    new = \"\" \n",
    "  \n",
    "    # traverse in the string  \n",
    "    for x in s: \n",
    "        new += x + ' '\n",
    "        a+=1\n",
    "  \n",
    "    # return string  \n",
    "    return new, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_cleared = []\n",
    "train_cleared = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_char = 0\n",
    "for i in range(0, len(biblioteca)):\n",
    "    texto = biblioteca[i]\n",
    "    data = split_pdf(texto)\n",
    "    data = first_clear(data)\n",
    "    d, a = convert(data)\n",
    "    \n",
    "    total_char += a\n",
    "    data_cleared.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "0\n",
      "546022\n"
     ]
    }
   ],
   "source": [
    "print(len(data_cleared))\n",
    "print(len(train_cleared))\n",
    "print(total_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "for i, _d in enumerate(data_cleared):\n",
    "    if i < num_data[0]:\n",
    "        label = 'As'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n",
    "    elif i < (num_data[0] + num_data[1]):\n",
    "        label = 'Ml'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n",
    "    elif i < (num_data[0] + num_data[1] + num_data[2]):\n",
    "        label = 'Bd'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n",
    "    else:\n",
    "        label = 'Hd'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(data_cleared)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tagged_data[0:(num_data[0]-7)] + tagged_data[(num_data[0]-1):(num_data[0] + num_data[1]-3)] \\\n",
    "+ tagged_data[(num_data[0] + num_data[1]):(num_data[0] + num_data[1] + num_data[2]-3)] + \\\n",
    "tagged_data[(num_data[0] + num_data[1] + num_data[2]):-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tagged_data[num_data[0]-6:num_data[0]] + tagged_data[(num_data[0] + num_data[1]-2):num_data[0] + num_data[1]] \\\n",
    "+ tagged_data[(num_data[0] + num_data[1] + num_data[2]-2):(num_data[0] + num_data[1] + num_data[2])] +\\\n",
    "tagged_data[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As_27']\n",
      "['As_28']\n",
      "['As_29']\n",
      "['As_30']\n",
      "['As_31']\n",
      "['As_32']\n",
      "['Ml_40']\n",
      "['Ml_41']\n",
      "['Bd_49']\n",
      "['Bd_50']\n",
      "['Hd_58']\n",
      "['Hd_59']\n"
     ]
    }
   ],
   "source": [
    "len(test_data)\n",
    "for i in range(0, len(test_data)):\n",
    "    print(test_data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(train_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "    model.train(train_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As_27']\n",
      "[('As_19', 0.46147218346595764), ('As_21', 0.45939168334007263), ('As_24', 0.45195454359054565), ('Ml_33', 0.4482228755950928), ('Bd_47', 0.44313478469848633), ('As_14', 0.4395579695701599), ('As_18', 0.43617409467697144), ('Bd_42', 0.4332016408443451), ('As_20', 0.42194288969039917), ('As_15', 0.4189050495624542)]\n",
      "['As_28']\n",
      "[('As_19', 0.4463167190551758), ('As_15', 0.4249621629714966), ('Ml_35', 0.4203965365886688), ('Bd_46', 0.41479042172431946), ('Ml_34', 0.410614550113678), ('Bd_42', 0.40969881415367126), ('Ml_33', 0.4039025902748108), ('As_21', 0.4037351608276367), ('Bd_47', 0.3969321846961975), ('As_18', 0.39616310596466064)]\n",
      "['As_29']\n",
      "[('As_19', 0.4678451418876648), ('As_14', 0.43469321727752686), ('As_18', 0.42145514488220215), ('Ml_34', 0.41988468170166016), ('Ml_33', 0.4135134220123291), ('As_21', 0.4119584560394287), ('Bd_44', 0.41059476137161255), ('Ml_36', 0.40933939814567566), ('Ml_35', 0.4059937596321106), ('Bd_42', 0.4007759392261505)]\n",
      "['As_30']\n",
      "[('As_19', 0.5071327686309814), ('Bd_47', 0.4548225700855255), ('As_14', 0.453178733587265), ('Ml_34', 0.45209020376205444), ('As_6', 0.4511750340461731), ('Ml_33', 0.42737719416618347), ('As_24', 0.4247656464576721), ('As_7', 0.4244998097419739), ('As_21', 0.42267149686813354), ('Hd_52', 0.4186529219150543)]\n",
      "['As_31']\n",
      "[('As_19', 0.45978808403015137), ('As_14', 0.44095927476882935), ('As_18', 0.4357084035873413), ('Ml_34', 0.4300571084022522), ('Ml_33', 0.42923611402511597), ('As_21', 0.4234009087085724), ('As_20', 0.4218754768371582), ('As_6', 0.4180660843849182), ('As_24', 0.4158681631088257), ('Bd_44', 0.4133060574531555)]\n",
      "['As_32']\n",
      "[('As_19', 0.49244123697280884), ('As_18', 0.46292972564697266), ('As_14', 0.45847368240356445), ('As_21', 0.45812875032424927), ('As_20', 0.45225298404693604), ('As_17', 0.43638670444488525), ('Ml_34', 0.43623077869415283), ('Ml_33', 0.4349808692932129), ('Ml_36', 0.4309394061565399), ('Bd_42', 0.43044957518577576)]\n",
      "['Ml_40']\n",
      "[('As_19', 0.46437376737594604), ('As_6', 0.44870099425315857), ('Ml_33', 0.44229164719581604), ('As_14', 0.43455439805984497), ('As_18', 0.4334535002708435), ('As_21', 0.43334445357322693), ('Bd_42', 0.4314950108528137), ('Ml_35', 0.42269933223724365), ('Bd_47', 0.4196773171424866), ('As_7', 0.4181991219520569)]\n",
      "['Ml_41']\n",
      "[('As_6', 0.5028213262557983), ('As_19', 0.49456140398979187), ('As_14', 0.4617950916290283), ('As_18', 0.45148739218711853), ('As_7', 0.44601914286613464), ('As_21', 0.4459373950958252), ('Hd_52', 0.44554048776626587), ('Bd_43', 0.4427259564399719), ('Ml_35', 0.44235607981681824), ('Bd_47', 0.44175437092781067)]\n",
      "['Bd_49']\n",
      "[('As_19', 0.48140794038772583), ('Bd_43', 0.4408414661884308), ('As_7', 0.4313816726207733), ('As_6', 0.4048106074333191), ('Ml_35', 0.4035969078540802), ('Ml_33', 0.40298575162887573), ('Ml_34', 0.4005849361419678), ('As_20', 0.40052637457847595), ('As_16', 0.3958722949028015), ('As_24', 0.392185777425766)]\n",
      "['Bd_50']\n",
      "[('As_19', 0.5266197919845581), ('As_6', 0.48864278197288513), ('As_14', 0.4878208041191101), ('As_21', 0.47620171308517456), ('Ml_34', 0.47293901443481445), ('As_20', 0.46305280923843384), ('As_18', 0.4577236473560333), ('As_17', 0.4565986394882202), ('Bd_47', 0.4520328938961029), ('Bd_44', 0.45163869857788086)]\n",
      "['Hd_58']\n",
      "[('As_19', 0.4767204821109772), ('As_21', 0.46744853258132935), ('As_14', 0.4522823691368103), ('As_18', 0.44455304741859436), ('Ml_33', 0.4442773461341858), ('Ml_34', 0.4409922957420349), ('Bd_47', 0.43827641010284424), ('As_20', 0.43170005083084106), ('As_24', 0.43083977699279785), ('As_15', 0.42776361107826233)]\n",
      "['Hd_59']\n",
      "[('As_19', 0.47479337453842163), ('As_21', 0.44725850224494934), ('As_20', 0.44190293550491333), ('Ml_34', 0.4373089671134949), ('Bd_47', 0.4236222505569458), ('Ml_33', 0.4169294238090515), ('As_18', 0.41229861974716187), ('As_14', 0.4102878272533417), ('As_17', 0.40732526779174805), ('As_15', 0.40452882647514343)]\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "\n",
    "for i in range(0, len(test_data)):\n",
    "    test = word_tokenize(str(test_data[i][0]))\n",
    "    v1 = model.infer_vector(test)\n",
    "    print(test_data[i][1])\n",
    "\n",
    "    # to find most similar doc using tags\n",
    "    similar_doc = model.docvecs.most_similar([v1])\n",
    "    print(similar_doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
