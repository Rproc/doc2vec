{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "import tika\n",
    "from tika import parser\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['data/Ml_', 'data/Bd_', 'data/Hd_', 'data/AS_']\n",
    "data = []\n",
    "num_data = []\n",
    "for elem in range(0, len(base)):\n",
    "    count = 0\n",
    "    final = 10    \n",
    "    for i in range(1, final):\n",
    "        terminal = base[elem] + str(i) + '.pdf'\n",
    "        data.append(terminal)\n",
    "        count+=1\n",
    "    \n",
    "    num_data.append(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Ml_1.pdf',\n",
       " 'data/Ml_2.pdf',\n",
       " 'data/Ml_3.pdf',\n",
       " 'data/Ml_4.pdf',\n",
       " 'data/Ml_5.pdf',\n",
       " 'data/Ml_6.pdf',\n",
       " 'data/Ml_7.pdf',\n",
       " 'data/Ml_8.pdf',\n",
       " 'data/Ml_9.pdf',\n",
       " 'data/Bd_1.pdf',\n",
       " 'data/Bd_2.pdf',\n",
       " 'data/Bd_3.pdf',\n",
       " 'data/Bd_4.pdf',\n",
       " 'data/Bd_5.pdf',\n",
       " 'data/Bd_6.pdf',\n",
       " 'data/Bd_7.pdf',\n",
       " 'data/Bd_8.pdf',\n",
       " 'data/Bd_9.pdf',\n",
       " 'data/Hd_1.pdf',\n",
       " 'data/Hd_2.pdf',\n",
       " 'data/Hd_3.pdf',\n",
       " 'data/Hd_4.pdf',\n",
       " 'data/Hd_5.pdf',\n",
       " 'data/Hd_6.pdf',\n",
       " 'data/Hd_7.pdf',\n",
       " 'data/Hd_8.pdf',\n",
       " 'data/Hd_9.pdf',\n",
       " 'data/AS_1.pdf',\n",
       " 'data/AS_2.pdf',\n",
       " 'data/AS_3.pdf',\n",
       " 'data/AS_4.pdf',\n",
       " 'data/AS_5.pdf',\n",
       " 'data/AS_6.pdf',\n",
       " 'data/AS_7.pdf',\n",
       " 'data/AS_8.pdf',\n",
       " 'data/AS_9.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['data/Ml_1.pdf', 'data/Ml_2.pdf', 'data/Ml_3.pdf', 'data/Ml_4.pdf', 'data/Ml_5.pdf', 'data/Ml_6.pdf', 'data/Ml_7.pdf']\n",
    "biblioteca = []\n",
    "tika.initVM()\n",
    "for i in range(0, len(data)):\n",
    "    \n",
    "    parsed = parser.from_file(data[i])\n",
    "    metadados = parsed[\"metadata\"]\n",
    "    texto = parsed[\"content\"]\n",
    "    biblioteca.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(biblioteca))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open (\"artigo2.txt\", \"w\")\n",
    "# f.write (str(texto))\n",
    "# f.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f = open (\"artigo2.txt\", \"r\")\n",
    "# texto = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_clear(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    #remove links and mark\n",
    "    data = [re.sub('(https:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(http:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(www.:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # remove spaces in begining and end\n",
    "    data = [re.sub(' +', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s): \n",
    "    a = 0\n",
    "    # initialization of string to \"\" \n",
    "    new = \"\" \n",
    "  \n",
    "    # traverse in the string  \n",
    "    for x in s: \n",
    "        new += x + ' '\n",
    "        a+=1\n",
    "  \n",
    "    # return string  \n",
    "    return new, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_cleared = []\n",
    "train_cleared = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_char = 0\n",
    "for i in range(0, len(biblioteca)):\n",
    "    texto = biblioteca[i]\n",
    "    data = split_pdf(texto)\n",
    "    data = first_clear(data)\n",
    "    d, a = convert(data)\n",
    "    \n",
    "    total_char += a\n",
    "    data_cleared.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0\n",
      "292303\n"
     ]
    }
   ],
   "source": [
    "print(len(data_cleared))\n",
    "print(len(train_cleared))\n",
    "print(total_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = []\n",
    "for i, _d in enumerate(data_cleared):\n",
    "    if i < num_data[0]:\n",
    "        label = 'Ml'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n",
    "    elif i < (num_data[0] + num_data[1]):\n",
    "        label = 'Bd'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n",
    "    elif i < (num_data[0] + num_data[1] + num_data[2]):\n",
    "        label = 'Hd'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n",
    "    else:\n",
    "        label = 'As'+ '_' + str(i)\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(_d), tags=[label]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ml_8']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(data_cleared)]\n",
    "# tagged_data[num_data[0]-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tagged_data[0:(num_data[0]-3)] + tagged_data[(num_data[0]):(num_data[0] + num_data[1]-3)] \\\n",
    "+ tagged_data[(num_data[0] + num_data[1]):(num_data[0] + num_data[1] + num_data[2]-3)] + \\\n",
    "tagged_data[(num_data[0] + num_data[1] + num_data[2]):-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tagged_data[num_data[0]-2:num_data[0]] + tagged_data[(num_data[0] + num_data[1]-2):num_data[0] + num_data[1]] \\\n",
    "+ tagged_data[(num_data[0] + num_data[1] + num_data[2]-2):(num_data[0] + num_data[1] + num_data[2])] +\\\n",
    "tagged_data[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ml_7']\n",
      "['Ml_8']\n",
      "['Bd_16']\n",
      "['Bd_17']\n",
      "['Hd_25']\n",
      "['Hd_26']\n",
      "['As_34']\n",
      "['As_35']\n"
     ]
    }
   ],
   "source": [
    "len(test_data)\n",
    "for i in range(0, len(test_data)):\n",
    "    print(test_data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(train_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "    model.train(train_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ml_7']\n",
      "[('Ml_1', 0.3849268853664398), ('Ml_5', 0.3821370601654053), ('As_32', 0.3779486119747162), ('Bd_14', 0.3774511218070984), ('Ml_0', 0.3771664500236511), ('Bd_9', 0.37615078687667847), ('Ml_2', 0.3696645498275757), ('Hd_19', 0.3638122081756592), ('Ml_3', 0.36351823806762695), ('Hd_23', 0.35977232456207275)]\n",
      "['Ml_8']\n",
      "[('Ml_5', 0.4376177191734314), ('Ml_1', 0.434550404548645), ('As_30', 0.4298199415206909), ('Hd_19', 0.4146783947944641), ('As_32', 0.41424503922462463), ('Ml_2', 0.4095132648944855), ('Bd_14', 0.40632909536361694), ('Bd_11', 0.4027211666107178), ('Ml_4', 0.3999013602733612), ('Bd_9', 0.3996428847312927)]\n",
      "['Bd_16']\n",
      "[('Ml_1', 0.3905556797981262), ('Hd_18', 0.39039361476898193), ('Bd_10', 0.3892677426338196), ('Hd_19', 0.37671351432800293), ('As_31', 0.3730244040489197), ('Ml_0', 0.36794954538345337), ('As_32', 0.36565977334976196), ('Ml_5', 0.36384570598602295), ('Ml_2', 0.356100857257843), ('Bd_9', 0.3545700013637543)]\n",
      "['Bd_17']\n",
      "[('Ml_1', 0.4538269340991974), ('Hd_19', 0.44480735063552856), ('Bd_14', 0.4378226399421692), ('As_30', 0.4261913299560547), ('Ml_3', 0.4244219660758972), ('Ml_4', 0.422441303730011), ('As_32', 0.42139679193496704), ('Bd_11', 0.4170082211494446), ('Ml_0', 0.41514310240745544), ('Ml_5', 0.4108542799949646)]\n",
      "['Hd_25']\n",
      "[('Hd_23', 0.4019976258277893), ('Ml_1', 0.39325660467147827), ('Bd_14', 0.3875144124031067), ('Ml_0', 0.37857556343078613), ('Ml_3', 0.37314146757125854), ('As_31', 0.368925005197525), ('Ml_5', 0.36490488052368164), ('Ml_4', 0.36071687936782837), ('As_32', 0.3600690960884094), ('Ml_2', 0.3544602692127228)]\n",
      "['Hd_26']\n",
      "[('Ml_1', 0.36689287424087524), ('Bd_14', 0.35671937465667725), ('Hd_18', 0.33827894926071167), ('Hd_19', 0.3377329409122467), ('Ml_3', 0.33510392904281616), ('Ml_0', 0.3326597511768341), ('Hd_23', 0.3305833041667938), ('Ml_2', 0.3305481970310211), ('Ml_5', 0.32497671246528625), ('Ml_4', 0.3221082091331482)]\n",
      "['As_34']\n",
      "[('Bd_13', 0.3709374666213989), ('Ml_1', 0.3477221727371216), ('Ml_5', 0.32388997077941895), ('Hd_18', 0.3169674277305603), ('Ml_3', 0.314512699842453), ('Ml_4', 0.31194373965263367), ('Hd_23', 0.30105364322662354), ('Ml_2', 0.30065393447875977), ('Bd_10', 0.29649651050567627), ('Bd_9', 0.2953909933567047)]\n",
      "['As_35']\n",
      "[('Ml_5', 0.3674762547016144), ('Ml_2', 0.3621615171432495), ('Bd_9', 0.3460967540740967), ('Hd_23', 0.34568944573402405), ('Ml_0', 0.3379788398742676), ('Ml_1', 0.3374105989933014), ('Ml_3', 0.3299121856689453), ('Ml_4', 0.32935166358947754), ('As_31', 0.31900554895401), ('Hd_18', 0.3181436061859131)]\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "\n",
    "for i in range(0, len(test_data)):\n",
    "    test = word_tokenize(str(test_data[i][0]))\n",
    "    v1 = model.infer_vector(test)\n",
    "    print(test_data[i][1])\n",
    "\n",
    "    # to find most similar doc using tags\n",
    "    similar_doc = model.docvecs.most_similar([v1])\n",
    "    print(similar_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
