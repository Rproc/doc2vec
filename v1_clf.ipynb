{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "import tika\n",
    "from tika import parser\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['data/AS_', 'data/Ml_', 'data/Bd_', 'data/Hd_']\n",
    "dados = []\n",
    "num_data = []\n",
    "for elem in range(0, len(base)):\n",
    "    count = 0\n",
    "    if elem != 0:\n",
    "        final = 10\n",
    "    else:\n",
    "        final = 34\n",
    "        \n",
    "    for i in range(1, final):\n",
    "        terminal = base[elem] + str(i) + '.pdf'\n",
    "        dados.append(terminal)\n",
    "        count+=1\n",
    "    \n",
    "    num_data.append(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/AS_1.pdf', 'data/AS_2.pdf', 'data/AS_3.pdf', 'data/AS_4.pdf', 'data/AS_5.pdf', 'data/AS_6.pdf', 'data/AS_7.pdf', 'data/AS_8.pdf', 'data/AS_9.pdf', 'data/AS_10.pdf', 'data/AS_11.pdf', 'data/AS_12.pdf', 'data/AS_13.pdf', 'data/AS_14.pdf', 'data/AS_15.pdf', 'data/AS_16.pdf', 'data/AS_17.pdf', 'data/AS_18.pdf', 'data/AS_19.pdf', 'data/AS_20.pdf', 'data/AS_21.pdf', 'data/AS_22.pdf', 'data/AS_23.pdf', 'data/AS_24.pdf', 'data/AS_25.pdf', 'data/AS_26.pdf', 'data/AS_27.pdf', 'data/AS_28.pdf', 'data/AS_29.pdf', 'data/AS_30.pdf', 'data/AS_31.pdf', 'data/AS_32.pdf', 'data/AS_33.pdf', 'data/Ml_1.pdf', 'data/Ml_2.pdf', 'data/Ml_3.pdf', 'data/Ml_4.pdf', 'data/Ml_5.pdf', 'data/Ml_6.pdf', 'data/Ml_7.pdf', 'data/Ml_8.pdf', 'data/Ml_9.pdf', 'data/Bd_1.pdf', 'data/Bd_2.pdf', 'data/Bd_3.pdf', 'data/Bd_4.pdf', 'data/Bd_5.pdf', 'data/Bd_6.pdf', 'data/Bd_7.pdf', 'data/Bd_8.pdf', 'data/Bd_9.pdf', 'data/Hd_1.pdf', 'data/Hd_2.pdf', 'data/Hd_3.pdf', 'data/Hd_4.pdf', 'data/Hd_5.pdf', 'data/Hd_6.pdf', 'data/Hd_7.pdf', 'data/Hd_8.pdf', 'data/Hd_9.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ['data/Ml_1.pdf', 'data/Ml_2.pdf', 'data/Ml_3.pdf', 'data/Ml_4.pdf', 'data/Ml_5.pdf', 'data/Ml_6.pdf', 'data/Ml_7.pdf']\n",
    "biblioteca = []\n",
    "tika.initVM()\n",
    "for i in range(0, len(dados)):\n",
    "    \n",
    "    parsed = parser.from_file(dados[i])\n",
    "    metadados = parsed[\"metadata\"]\n",
    "    texto = parsed[\"content\"]\n",
    "    biblioteca.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(biblioteca))\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open (\"artigo2.txt\", \"w\")\n",
    "# f.write (str(texto))\n",
    "# f.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f = open (\"artigo2.txt\", \"r\")\n",
    "# texto = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_clear(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    #remove links and mark\n",
    "    data = [re.sub('(https:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(http:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "    \n",
    "    data = [re.sub('(www.:\\S+)|(@)|(¿)', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # remove spaces in begining and end\n",
    "    data = [re.sub(' +', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s): \n",
    "    a = 0\n",
    "    # initialization of string to \"\" \n",
    "    new = \"\" \n",
    "  \n",
    "    # traverse in the string  \n",
    "    for x in s: \n",
    "        new += x + ' '\n",
    "        a+=1\n",
    "  \n",
    "    # return string  \n",
    "    return new, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleared = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_char = 0\n",
    "for i in range(0, len(biblioteca)):\n",
    "    texto = biblioteca[i]\n",
    "    data = split_pdf(texto)\n",
    "    data = first_clear(data)\n",
    "    d, a = convert(data)\n",
    "    \n",
    "    total_char += a\n",
    "    data_cleared.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "546022\n"
     ]
    }
   ],
   "source": [
    "print(len(data_cleared))\n",
    "print(total_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_cleared)\n",
    "df = df.rename(columns={0: \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accurate Methods for the Statistics of Surpri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/17/2019 A Companion to Digital Literary St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fqi063 311..326 A Tool for Literary Studies: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A survey of modern authorship attribution met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linguistic Society of America is collaborat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   Accurate Methods for the Statistics of Surpri...\n",
       "1   11/17/2019 A Companion to Digital Literary St...\n",
       "2   fqi063 311..326 A Tool for Literary Studies: ...\n",
       "3   A survey of modern authorship attribution met...\n",
       "4     Linguistic Society of America is collaborat..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(0, len(dados)):\n",
    "    if 'AS' in str(dados[i]):\n",
    "        l.append('Analyze_Statistics')\n",
    "    elif 'Ml' in str(dados[i]):\n",
    "        l.append('Machine_Learning')\n",
    "    elif 'Bd' in str(dados[i]):\n",
    "        l.append('Big_Data')\n",
    "    else:\n",
    "        l.append('Digital_Humanities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classe'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accurate Methods for the Statistics of Surpri...</td>\n",
       "      <td>Analyze_Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/17/2019 A Companion to Digital Literary St...</td>\n",
       "      <td>Analyze_Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fqi063 311..326 A Tool for Literary Studies: ...</td>\n",
       "      <td>Analyze_Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A survey of modern authorship attribution met...</td>\n",
       "      <td>Analyze_Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linguistic Society of America is collaborat...</td>\n",
       "      <td>Analyze_Statistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              classe\n",
       "0   Accurate Methods for the Statistics of Surpri...  Analyze_Statistics\n",
       "1   11/17/2019 A Companion to Digital Literary St...  Analyze_Statistics\n",
       "2   fqi063 311..326 A Tool for Literary Studies: ...  Analyze_Statistics\n",
       "3   A survey of modern authorship attribution met...  Analyze_Statistics\n",
       "4     Linguistic Society of America is collaborat...  Analyze_Statistics"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.classe]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.classe]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31    ([fqm012, 345..362, delta, for, middle, dutch—...\n",
       "3     ([survey, of, modern, authorship, attribution,...\n",
       "52    ([9-25_1, golumbia, 156-76, .indd, volume, 25,...\n",
       "17    ([corpus, linguistics, and, the, study, of, li...\n",
       "8     ([corpus, 2003, corpus, 2003, la, distance, in...\n",
       "6     ([journal, of, machine, learning, research, 20...\n",
       "40    ([large-scale, classification, of, fine-art, p...\n",
       "4     ([linguistic, society, of, america, is, collab...\n",
       "43    ([microsoft, word, forman-jmlr-fxselectionstud...\n",
       "19    ([bayesian, analysis, of, multinomial, sequenc...\n",
       "34    ([fqs003, 183..196, detecting, authorship, dec...\n",
       "58    ([11/8/12, dhq, digital, humanities, quarterly...\n",
       "25    ([digital, humanities, 2010, does, size, matte...\n",
       "56    ([signs, symbols, and, discourses, new, direct...\n",
       "15    ([11/17/2019, clustering, with, compression, f...\n",
       "27    ([11/17/2019, from, babel, to, knowledge, data...\n",
       "9     ([authorial, attribution, and, computational, ...\n",
       "30    ([dating, medieval, english, charters, the, an...\n",
       "26    ([from, opportunistic, to, systematic, use, of...\n",
       "16    ([op-llcj130024, 563..575, automatic, recognit...\n",
       "24    ([full, terms, conditions, of, access, and, us...\n",
       "55    ([fqm042, 73..84, killer, applications, in, di...\n",
       "11    ([11/17/2019, cultural, goldmine, lurks, in, d...\n",
       "32    ([language, and, literature, the, online, vers...\n",
       "53    ([distant, reading, and, discourse, analysis, ...\n",
       "41    ([acmcs00.tex, machine, learning, in, automate...\n",
       "37    ([fqi039, 259..274, new, approach, to, the, st...\n",
       "29    ([fqr031, 315..322, deeper, delta, across, gen...\n",
       "44    ([fqs010, 139..154, automatic, prediction, of,...\n",
       "1     ([11/17/2019, companion, to, digital, literary...\n",
       "21    ([keywords-culpeper, jonathan, culpeper, 11, t...\n",
       "2     ([fqi063, 311..326, tool, for, literary, studi...\n",
       "47    ([probabilistic, topic, models, review, articl...\n",
       "39    ([op-llcj120026, 229..236, do, birds, of, feat...\n",
       "35    ([new, machine, learning, methods, demonstrate...\n",
       "23    ([lit, ling, 18/2, 129-138, final, literary, a...\n",
       "49    ([pone.0066813, 1..12, language, individuation...\n",
       "10    ([11/17/2019, victorian, literature, statistic...\n",
       "22    ([llc_27_2reviews, 227..238, gitelman, l., 200...\n",
       "18    ([conrad, in, the, computer, examples, of, qua...\n",
       "59    ([burns—documents—ade, 150, 2010, file, kirsch...\n",
       "20    ([journal, of, the, text, encoding, initiative...\n",
       "7     ([microsoft, word, dabagh.doc, metodološki, zv...\n",
       "42    ([unified, approach, to, mapping, and, cluster...\n",
       "14    ([corpus, stylistics, speech, writing, and, th...\n",
       "28    ([litlin, 19_4, 477-495, fqh035, fin, literary...\n",
       "51    ([confronting, the, digital, or, how, academic...\n",
       "38    ([using, markov, chains, for, identification, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 97921.49it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=1, vector_size=500, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 6666.00it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 115838.09it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 165428.59it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 196608.00it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 270964.46it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 284761.80it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 41103.84it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 34651.74it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 173407.92it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 63751.30it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 196992.75it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 76871.55it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 167214.78it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 395533.58it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 166248.22it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 163946.74it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 290514.56it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 196992.75it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 205855.41it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 72082.56it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 157409.38it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 231943.08it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 115838.09it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 211477.51it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 200125.84it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 200524.49it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 196224.75it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 198742.93it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 167912.09it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 174459.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.9 s, sys: 333 ms, total: 40.2 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5\n",
      "Testing F1 score: 0.42735042735042733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
